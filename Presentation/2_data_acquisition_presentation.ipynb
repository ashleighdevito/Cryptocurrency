{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.10 64-bit ('PythonData': conda)"
  },
  "interpreter": {
   "hash": "ed99c328fe6228cde88a5ea9dac3573c8a80070b6f57470153ed5be436ecc5ea"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime as dt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Up to time 1527830059\n",
      "Up to time 1527834608\n",
      "Up to time 1527838634\n",
      "Up to time 1527842009\n",
      "Up to time 1527846154\n",
      "Up to time 1527850779\n",
      "Up to time 1527855059\n",
      "Up to time 1527858922\n",
      "Up to time 1527862277\n",
      "Up to time 1527865490\n",
      "Up to time 1527868144\n",
      "Up to time 1527871031\n",
      "Up to time 1527873622\n",
      "Up to time 1527876281\n",
      "Up to time 1527878618\n",
      "Up to time 1527881030\n",
      "Up to time 1527883424\n",
      "Up to time 1527886357\n",
      "Up to time 1527888915\n",
      "Up to time 1527891868\n",
      "Up to time 1527894653\n",
      "Up to time 1527897802\n",
      "Up to time 1527901441\n",
      "Up to time 1527905693\n",
      "Up to time 1527909836\n",
      "Up to time 1527915007\n",
      "Up to time 1527920744\n",
      "Up to time 1527925373\n",
      "Up to time 1527929982\n",
      "Up to time 1527934327\n",
      "Up to time 1527938630\n",
      "Up to time 1527942122\n",
      "Up to time 1527945529\n",
      "Up to time 1527948728\n",
      "Up to time 1527951150\n",
      "Up to time 1527953819\n",
      "Up to time 1527957092\n",
      "Up to time 1527959429\n",
      "Up to time 1527961916\n",
      "Up to time 1527964693\n",
      "Up to time 1527967893\n",
      "Up to time 1527972137\n",
      "Up to time 1527975763\n",
      "Up to time 1527979684\n",
      "Up to time 1527984383\n",
      "Up to time 1527989007\n",
      "Up to time 1527993042\n",
      "Up to time 1527997089\n",
      "Up to time 1528001849\n",
      "Up to time 1528007018\n",
      "Up to time 1528012591\n",
      "Up to time 1528017963\n",
      "Up to time 1528022343\n",
      "Up to time 1528027643\n",
      "Up to time 1528031457\n",
      "Up to time 1528034942\n",
      "Up to time 1528037934\n",
      "Up to time 1528041027\n",
      "Up to time 1528044414\n",
      "Up to time 1528047468\n",
      "Up to time 1528050229\n",
      "Up to time 1528053151\n",
      "Up to time 1528056250\n",
      "Up to time 1528058846\n",
      "Up to time 1528062321\n",
      "Up to time 1528066449\n",
      "Up to time 1528070491\n",
      "Up to time 1528075251\n",
      "Up to time 1528079546\n",
      "Up to time 1528084137\n",
      "Up to time 1528087749\n",
      "Up to time 1528092491\n",
      "Up to time 1528096343\n",
      "Up to time 1528101162\n",
      "Up to time 1528106211\n",
      "Up to time 1528110201\n",
      "Up to time 1528115250\n",
      "Up to time 1528118044\n",
      "Up to time 1528120878\n",
      "Up to time 1528123635\n",
      "Up to time 1528126905\n",
      "Up to time 1528129427\n",
      "Up to time 1528131933\n",
      "Up to time 1528134777\n",
      "Up to time 1528137500\n",
      "Up to time 1528139941\n",
      "Up to time 1528142304\n",
      "Up to time 1528145400\n",
      "Up to time 1528147891\n",
      "Up to time 1528150342\n",
      "Up to time 1528153154\n",
      "Up to time 1528157178\n",
      "Up to time 1528161129\n",
      "Up to time 1528164255\n",
      "Up to time 1528168672\n",
      "Up to time 1528173043\n",
      "Up to time 1528177676\n",
      "Up to time 1528182484\n",
      "Up to time 1528187205\n",
      "Up to time 1528191752\n",
      "Up to time 1528196551\n",
      "Up to time 1528200777\n",
      "Up to time 1528204734\n",
      "Up to time 1528207544\n",
      "Up to time 1528210254\n",
      "Up to time 1528212864\n",
      "Up to time 1528215076\n",
      "Up to time 1528217323\n",
      "Up to time 1528219804\n",
      "Up to time 1528222703\n",
      "Up to time 1528224784\n",
      "Up to time 1528227549\n",
      "Up to time 1528230208\n",
      "Up to time 1528233005\n",
      "Up to time 1528235757\n",
      "Up to time 1528238702\n",
      "Up to time 1528242003\n",
      "Up to time 1528246902\n",
      "Up to time 1528251262\n",
      "Up to time 1528255850\n",
      "Up to time 1528258844\n",
      "Up to time 1528262464\n",
      "Up to time 1528266684\n",
      "Up to time 1528270779\n",
      "Up to time 1528274994\n",
      "Up to time 1528279063\n",
      "Up to time 1528283508\n",
      "Up to time 1528288037\n",
      "Up to time 1528291348\n",
      "Up to time 1528293815\n",
      "Up to time 1528296018\n",
      "Up to time 1528298549\n",
      "Up to time 1528301452\n",
      "Up to time 1528303870\n",
      "Up to time 1528306474\n",
      "Up to time 1528308915\n",
      "Up to time 1528310863\n",
      "Up to time 1528313453\n",
      "Up to time 1528316147\n",
      "Up to time 1528318744\n",
      "Up to time 1528321574\n",
      "Up to time 1528323989\n",
      "Up to time 1528327618\n",
      "Up to time 1528331066\n",
      "Up to time 1528334922\n",
      "Up to time 1528338255\n",
      "Up to time 1528343385\n",
      "Up to time 1528347693\n",
      "Up to time 1528352200\n",
      "Up to time 1528357217\n",
      "Up to time 1528361240\n",
      "Up to time 1528366214\n",
      "Up to time 1528370605\n",
      "Up to time 1528373865\n",
      "Up to time 1528376384\n",
      "Up to time 1528378779\n",
      "Up to time 1528381620\n",
      "Up to time 1528383791\n",
      "Up to time 1528386131\n",
      "Up to time 1528388045\n",
      "Up to time 1528390086\n",
      "Up to time 1528392700\n",
      "Up to time 1528396124\n",
      "Up to time 1528399600\n",
      "Up to time 1528401992\n",
      "Up to time 1528405154\n",
      "Up to time 1528407729\n",
      "Up to time 1528410942\n",
      "Up to time 1528414693\n",
      "Up to time 1528419493\n",
      "Up to time 1528423966\n",
      "Up to time 1528428560\n",
      "Up to time 1528433432\n",
      "Up to time 1528439291\n",
      "Up to time 1528444864\n",
      "Up to time 1528450739\n",
      "Up to time 1528456205\n",
      "Up to time 1528460583\n",
      "Up to time 1528464127\n",
      "Up to time 1528467102\n",
      "Up to time 1528470363\n",
      "Up to time 1528472836\n",
      "Up to time 1528475970\n",
      "Up to time 1528478339\n",
      "Up to time 1528481704\n",
      "Up to time 1528483835\n",
      "Up to time 1528486476\n",
      "Up to time 1528488919\n",
      "Up to time 1528492401\n",
      "Up to time 1528496166\n",
      "Up to time 1528499689\n",
      "Up to time 1528503873\n",
      "Up to time 1528508392\n",
      "Up to time 1528512770\n",
      "Up to time 1528516913\n",
      "Up to time 1528522166\n",
      "Up to time 1528527064\n",
      "Up to time 1528531713\n",
      "Up to time 1528536783\n",
      "Up to time 1528540794\n",
      "Up to time 1528545230\n",
      "Up to time 1528549372\n",
      "Up to time 1528552996\n",
      "Up to time 1528555750\n",
      "Up to time 1528558482\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-be715078fe19>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mi\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m#sleep for one second\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;31m#if the last comment was posted before the end of our time frame of interest, keep calling for more data starting at the last time collected\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#set timeframe of data acquisition\n",
    "start = 1527825600\n",
    "end = 1593576000\n",
    "\n",
    "#initalize starting bound\n",
    "bound = start\n",
    "\n",
    "#create dictionary to store gathered data\n",
    "bitcoin_comment_times = {'Timestamp' : [], 'SubReddit' : []}\n",
    "\n",
    "#create while loop to gather all comments containing the keyword within the time frame\n",
    "while bound < end:\n",
    "    url = f\"https://api.pushshift.io/reddit/search/comment/?size=100&after={bound}&before={end}&q=Bitcoin\"\n",
    "    crypto_comment_batch = requests.get(url).json()\n",
    "#identify how many comments are pulled in that api call batch\n",
    "    set_length = len(crypto_comment_batch['data'])\n",
    "    i = 0\n",
    "#step through each comment to obtain the time it was created and the subreddit in which it was posted\n",
    "    while i < set_length:\n",
    "        bitcoin_comment_times['Timestamp'].append(crypto_comment_batch['data'][i]['created_utc'])\n",
    "        bitcoin_comment_times['SubReddit'].append(crypto_comment_batch['data'][i]['subreddit'])\n",
    "        i += 1\n",
    "#sleep for one second\n",
    "    time.sleep(1)\n",
    "#if the last comment was posted before the end of our time frame of interest, keep calling for more data starting at the last time collected\n",
    "    try:\n",
    "        bound = crypto_comment_batch['data'][set_length-1]['created_utc']\n",
    "    except:\n",
    "        bound = end\n",
    "#report progress\n",
    "    print(f'Up to time {bound}')\n",
    "\n",
    "#create dataframe from the comment dictionary\n",
    "bitcoin_comment_df = pd.DataFrame(bitcoin_comment_times, columns = ['Timestamp', 'SubReddit'])\n",
    "\n",
    "#save to csv\n",
    "bitcoin_comment_df.to_csv(\"output/comments_bitcoin.csv\")\n",
    "\n",
    "#notify of completion\n",
    "print('acquisition complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#repeat process for ethereum\n",
    "start = 1527825600\n",
    "end = 1593576000\n",
    "\n",
    "bound = start\n",
    "\n",
    "ethereum_comment_times = {'Timestamp' : [], 'SubReddit' : []}\n",
    "\n",
    "while bound < end:\n",
    "    url = f\"https://api.pushshift.io/reddit/search/comment/?size=100&after={bound}&before={end}&q=Ethereum\"\n",
    "    crypto_comment_batch = requests.get(url).json()\n",
    "    set_length = len(crypto_comment_batch['data'])\n",
    "    i = 0\n",
    "    while i < set_length:\n",
    "        ethereum_comment_times['Timestamp'].append(crypto_comment_batch['data'][i]['created_utc'])\n",
    "        ethereum_comment_times['SubReddit'].append(crypto_comment_batch['data'][i]['subreddit'])\n",
    "        i += 1\n",
    "    time.sleep(1)\n",
    "    try:\n",
    "        bound = crypto_comment_batch['data'][set_length-1]['created_utc']\n",
    "    except:\n",
    "        bound = end\n",
    "    print(f'Up to time {bound}')\n",
    "\n",
    "ethereum_comment_df = pd.DataFrame(ethereum_comment_times, columns = ['Timestamp', 'SubReddit'])\n",
    "ethereum_comment_df.to_csv(\"output/comments_ethereum.csv\")\n",
    "print('acquisition complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#repeat process for dogecoin\n",
    "start = 1527825600\n",
    "end = 1593576000\n",
    "\n",
    "bound = start\n",
    "\n",
    "dogecoin_comment_times = {'Timestamp' : [], 'SubReddit' : []}\n",
    "\n",
    "while bound < end:\n",
    "    url = f\"https://api.pushshift.io/reddit/search/comment/?size=100&after={bound}&before={end}&q=Dogecoin\"\n",
    "    crypto_comment_batch = requests.get(url).json()\n",
    "    set_length = len(crypto_comment_batch['data'])\n",
    "    i = 0\n",
    "    while i < set_length:\n",
    "        dogecoin_comment_times['Timestamp'].append(crypto_comment_batch['data'][i]['created_utc'])\n",
    "        dogecoin_comment_times['SubReddit'].append(crypto_comment_batch['data'][i]['subreddit'])\n",
    "        i += 1\n",
    "    time.sleep(1)\n",
    "    try:\n",
    "        bound = crypto_comment_batch['data'][set_length-1]['created_utc']\n",
    "    except:\n",
    "        bound = end\n",
    "    print(f'Up to time {bound}')\n",
    "\n",
    "dogecoin_comment_df = pd.DataFrame(dogecoin_comment_times, columns = ['Timestamp', 'SubReddit'])\n",
    "dogecoin_comment_df.to_csv(\"output/comments_dogecoin.csv\")\n",
    "print('acquisition complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#repeat process for cardano\n",
    "start = 1527825600\n",
    "end = 1593576000\n",
    "\n",
    "bound = start\n",
    "\n",
    "cardano_comment_times = {'Timestamp' : [], 'SubReddit' : []}\n",
    "\n",
    "while bound < end:\n",
    "    url = f\"https://api.pushshift.io/reddit/search/comment/?size=100&after={bound}&before={end}&q=Cardano\"\n",
    "    crypto_comment_batch = requests.get(url).json()\n",
    "    set_length = len(crypto_comment_batch['data'])\n",
    "    i = 0\n",
    "    while i < set_length:\n",
    "        cardano_comment_times['Timestamp'].append(crypto_comment_batch['data'][i]['created_utc'])\n",
    "        cardano_comment_times['SubReddit'].append(crypto_comment_batch['data'][i]['subreddit'])\n",
    "        i += 1\n",
    "    time.sleep(1)\n",
    "    try:\n",
    "        bound = crypto_comment_batch['data'][set_length-1]['created_utc']\n",
    "    except:\n",
    "        bound = end\n",
    "    print(f'Up to time {bound}')\n",
    "\n",
    "cardano_comment_df = pd.DataFrame(cardano_comment_times, columns = ['Timestamp', 'SubReddit'])\n",
    "cardano_comment_df.to_csv(\"output/comments_cardano.csv\")\n",
    "print('acquisition complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 1527825600\n",
    "end = 1593576000\n",
    "\n",
    "bound = start\n",
    "\n",
    "NASDAQ_comment_times = {'Timestamp' : [], 'SubReddit' : []}\n",
    "\n",
    "while bound < end:\n",
    "    url = f\"https://api.pushshift.io/reddit/search/comment/?size=100&after={bound}&before={end}&q=NASDAQ\"\n",
    "    stock_comment_batch = requests.get(url).json()\n",
    "    set_length = len(stock_comment_batch['data'])\n",
    "    i = 0\n",
    "    while i < set_length:\n",
    "        NASDAQ_comment_times['Timestamp'].append(stock_comment_batch['data'][i]['created_utc'])\n",
    "        NASDAQ_comment_times['SubReddit'].append(stock_comment_batch['data'][i]['subreddit'])\n",
    "        i += 1\n",
    "    time.sleep(1)\n",
    "    try:\n",
    "        bound = stock_comment_batch['data'][set_length-1]['created_utc']\n",
    "    except:\n",
    "        bound = end\n",
    "    print(f'Up to time {bound}')\n",
    "\n",
    "stock_comment_df = pd.DataFrame(NASDAQ_comment_times, columns = ['Timestamp', 'SubReddit'])\n",
    "stock_comment_df.to_csv(\"output/comments_NASDAQ.csv\")\n",
    "print('acquisition complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 1527825600\n",
    "end = 1593576000\n",
    "\n",
    "bound = start\n",
    "\n",
    "DOW_comment_times = {'Timestamp' : [], 'SubReddit' : []}\n",
    "\n",
    "while bound < end:\n",
    "    url = f\"https://api.pushshift.io/reddit/search/comment/?size=100&after={bound}&before={end}&q=DOW\"\n",
    "    stock_comment_batch = requests.get(url).json()\n",
    "    set_length = len(stock_comment_batch['data'])\n",
    "    i = 0\n",
    "    while i < set_length:\n",
    "        DOW_comment_times['Timestamp'].append(stock_comment_batch['data'][i]['created_utc'])\n",
    "        DOW_comment_times['SubReddit'].append(stock_comment_batch['data'][i]['subreddit'])\n",
    "        i += 1\n",
    "    time.sleep(1)\n",
    "    try:\n",
    "        bound = stock_comment_batch['data'][set_length-1]['created_utc']\n",
    "    except:\n",
    "        bound = end\n",
    "    print(f'Up to time {bound}')\n",
    "\n",
    "stock_comment_df = pd.DataFrame(DOW_comment_times, columns = ['Timestamp', 'SubReddit'])\n",
    "stock_comment_df.to_csv(\"output/comments_DOW.csv\")\n",
    "print('acquisition complete')"
   ]
  }
 ]
}